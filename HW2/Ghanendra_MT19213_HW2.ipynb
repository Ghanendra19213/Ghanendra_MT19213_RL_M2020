{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RL HW2 MT19213 Ghanendra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 2. Write code that solves the linear equations required to find v π ( s ) and generate the values in the table in Figure 3.2 Note that the policy π picks all valid actions in a state with equal probability. Add comments to your code that explain all your steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state:  [1 1]\n",
      "\n",
      "Gridworld state value function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 3.3,  8.8,  4.4,  5.3,  1.5],\n",
       "       [ 1.5,  3. ,  2.3,  1.9,  0.5],\n",
       "       [ 0.1,  0.7,  0.7,  0.4, -0.4],\n",
       "       [-1. , -0.4, -0.4, -0.6, -1.2],\n",
       "       [-1.9, -1.3, -1.2, -1.4, -2. ]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gridworld\n",
    "# Possible actions (north, south, east, and west)\n",
    "# Possible state (location or cells in the grid)\n",
    "# Considering equiprobable random policy for selection each action with probability 0.25\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "\n",
    "G = np.zeros([5,5]) #Grid of size 5x5\n",
    "Y = 0.9 # gamma\n",
    "r = 0 # Initial reward = 0\n",
    "# Initial state selecting randomly\n",
    "x = [0,1,2,3,4] # array for selecting state\n",
    "actions = ['n','s','e','w'] # action array\n",
    "init = np.random.choice(x,2) # randomly select any state\n",
    "print('Initial state: ',init)\n",
    "ix = init[0] # x pos of agent in grid\n",
    "iy = init[1] # y pos of agent in grid\n",
    "\n",
    "# Function for calculating state value function\n",
    "def vpi_s(action):\n",
    "    global ix\n",
    "    global iy\n",
    "    global r\n",
    "    # For state A making transition to A'\n",
    "    if (ix==0 and iy ==1):\n",
    "        r = 10 # Reward\n",
    "        # Expected value for all the four actions\n",
    "        #G[ix][iy] = 0.25*(r + Y*G[4][1])*4\n",
    "        G[ix][iy] = (r + Y*G[4][1])\n",
    "        ix,iy = 4,1 # Updated position\n",
    "\n",
    "    # For state B making transition to B'\n",
    "    elif (ix==0 and iy ==3):\n",
    "        r = 5 # Reward\n",
    "        # Expected value for all the four actions\n",
    "        #G[ix][iy] = 0.25*(r + Y*G[4][1])*4\n",
    "        G[ix][iy] = r + Y*G[2][3]\n",
    "        ix,iy = 2,3 # Updated position\n",
    "    else:\n",
    "        # 0 if hit the grid else grid value\n",
    "        n = G[ix-1][iy] if ix>0 else 0\n",
    "        s = G[ix+1][iy] if ix<4 else 0\n",
    "        e = G[ix][iy+1] if iy<4 else 0\n",
    "        w = G[ix][iy-1] if iy>0 else 0        \n",
    "        s = [n,s,e,w] # storing in array\n",
    "\n",
    "        s_val = 0 # state value\n",
    "        #s_d = update_pos(action)\n",
    "        s_d = G[ix][iy]\n",
    "        # Calculate average from neighboring states\n",
    "        for i in s:\n",
    "            # Action within the grid\n",
    "            if i!=0:\n",
    "                r=0\n",
    "                s_val += 0.25*(r + Y*i)\n",
    "            #Actions at edge in grid\n",
    "            else:\n",
    "                r=-1\n",
    "                s_val += 0.25*(r + Y*s_d)\n",
    "        G[ix][iy] = s_val\n",
    "\n",
    "    # Update the position in other states\n",
    "    update_pos(action)\n",
    "    # For every instance return grid value\n",
    "    return G\n",
    "\n",
    "# Function to update the position based on action\n",
    "def update_pos(action):\n",
    "    global ix\n",
    "    global iy\n",
    "        \n",
    "    if (action =='n'): # north\n",
    "        if (ix>0):\n",
    "            ix=ix-1\n",
    "    elif (action =='s'): # south\n",
    "        if (ix<4):\n",
    "            ix=ix+1\n",
    "    elif (action =='e'): # east\n",
    "        if (iy<4):\n",
    "            iy=iy+1\n",
    "    elif (action =='w'): # west\n",
    "        if (iy>0):\n",
    "            iy=iy-1     \n",
    "    \n",
    "t = 10000 # No. of iterations to converge effectively\n",
    "for i in range(t):\n",
    "    # a is the action choosen randomly at time t\n",
    "    a = np.random.choice(actions,1)\n",
    "    #s(t) is the state at time t\n",
    "    s_t = vpi_s(a) # Call vpi(s) function\n",
    "\n",
    "print('\\nGridworld state value function')\n",
    "np.round(G,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 3. Solve Exercises 3.15 and 3.16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.15\n",
    "\n",
    "Yes the signs are important because the state A with reward +10 was better compared to state B with reward +5. Second, the sign of A' state which is more negative than B' state indicates its less preferred state to achieve the goal. Third reward with different sign determines action selection for a better policy and goal directed behavior.\n",
    "\n",
    "##### Proof using Equation 3.8. adding a constant c to each reward at time step t, we have Y is (gamma)discounting factor, expected return is given by\n",
    "\n",
    "G(t) = R(t+1)+c + Y*[R(t+2)+c] + Y^2 *[R(t+3) + c] + Y^3 * [R(t+4) + c] + . . . . . . \n",
    "\n",
    "###### G(t) = R(t+1) + c + Y*R(t+2)+ Y*c + Y^2*R(t+3) + Y^2*c + Y^3*R(t+4) + Y^3*c + . . . .\n",
    "###### G(t) =  R(t+1) + Y*R(t+2) + Y^2*R(t+3) + Y^3*R(t+4) .....  + c + Y*c + Y^2*c + Y^3*c + ......\n",
    "###### G(t) =  R(t+1) + Y*R(t+2) + Y^2*R(t+3) + Y^3*R(t+4) .....  + c(1 + Y + Y^2 + Y^3 + ......)\n",
    "\n",
    "##### Taking c as common in the second expression we have \n",
    "\n",
    "G(t) = Summation[k=0 to inf] Y^k * R(t+k+1)   +   c * Summation [k=0 to inf] Y^k\n",
    "\n",
    "##### Normally we have 0<= Y <=1, if Y is a constant value < 1 , then second term is sum to infinity, therefore\n",
    "\n",
    "G(t) = Summation[k=0 to inf] Y^k * R(t+k+1)   +   c * 1/( 1 - Y) , where second term evaluates to a constant\n",
    "\n",
    "Hence, replacing c * 1/( 1 - Y) by a constant Vc.\n",
    "Now the equation is \n",
    "G(t) = Summation[k=0 to inf] Y^k * R(t+k+1)   +  Vc\n",
    "##### For simplicity we can take first term as Rk(t) = Summation[k=0 to inf] Y^k * R(t+k+1), note Rk(t) is not a constant\n",
    "G(t) = Rk(t) + Vc\n",
    "\n",
    "Now value function is given by\n",
    "\n",
    "vpi(s) = E(pi)[G(t) | S(t) = s] = E(pi) [ Rk(t) + Vc | S(t) = s] , for all small s belongs to state set capital S.\n",
    "vpi(s) = E(pi)[ Rk(t)| S(t)=s] + E(pi)[ Vc | S(t) = s]\n",
    "\n",
    "##### As Vc is a contant, expectation of a constant is constant and can be taken out, therefore\n",
    "vpi(s) = E(pi)[ Rk(t)| S(t)=s] + Vc\n",
    "\n",
    "##### Vc in terms of c and Y is given by\n",
    "#### Vc = c/(1-Y)\n",
    "\n",
    "Hence proved that adding a constant c to all rewards adds a constant Vc to the values of all states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 5. Given an equation for v ∗ in terms of q ∗ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v*(s) = max Summation[s',r] p(s',r|s,a) [r + Y* max q*(s',a')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 6. Code policy iteration and value iteration (VI) to solve the Gridworld in Example 4 . 1. Your code must log output of each iteration. Pick up a few sample iterations to show policy evaluation and improvement at work. Similarly, show using a few obtained iterations that every iteration of VI improves the value function. Your code must include the fix to the bug mentioned in Exercise 4 . 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Delta:  1.0\n",
      "\n",
      "After each iteration time step 1 V:\n",
      " [[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "\n",
      "Delta:  1.0\n",
      "\n",
      "After each iteration time step 2 V:\n",
      " [[ 0.  -1.8 -2.  -2. ]\n",
      " [-1.8 -2.  -2.  -2. ]\n",
      " [-2.  -2.  -2.  -1.8]\n",
      " [-2.  -2.  -1.8  0. ]]\n",
      "\n",
      "Delta:  1.0\n",
      "\n",
      "After each iteration time step 3 V:\n",
      " [[ 0.  -2.4 -2.9 -3. ]\n",
      " [-2.4 -2.9 -3.  -2.9]\n",
      " [-2.9 -3.  -2.9 -2.4]\n",
      " [-3.  -2.9 -2.4  0. ]]\n",
      "\n",
      "Delta:  0.96875\n",
      "\n",
      "After each iteration time step 4 V:\n",
      " [[ 0.  -3.1 -3.8 -4. ]\n",
      " [-3.1 -3.7 -3.9 -3.8]\n",
      " [-3.8 -3.9 -3.7 -3.1]\n",
      " [-4.  -3.8 -3.1  0. ]]\n",
      "\n",
      "Delta:  0.9375\n",
      "\n",
      "After each iteration time step 5 V:\n",
      " [[ 0.  -3.7 -4.7 -4.9]\n",
      " [-3.7 -4.5 -4.8 -4.7]\n",
      " [-4.7 -4.8 -4.5 -3.7]\n",
      " [-4.9 -4.7 -3.7  0. ]]\n",
      "\n",
      "Delta:  0.89453125\n",
      "\n",
      "After each iteration time step 6 V:\n",
      " [[ 0.  -4.2 -5.5 -5.8]\n",
      " [-4.2 -5.2 -5.6 -5.5]\n",
      " [-5.5 -5.6 -5.2 -4.2]\n",
      " [-5.8 -5.5 -4.2  0. ]]\n",
      "\n",
      "Delta:  0.8544921875\n",
      "\n",
      "After each iteration time step 7 V:\n",
      " [[ 0.  -4.7 -6.3 -6.7]\n",
      " [-4.7 -5.9 -6.4 -6.3]\n",
      " [-6.3 -6.4 -5.9 -4.7]\n",
      " [-6.7 -6.3 -4.7  0. ]]\n",
      "\n",
      "Delta:  0.81103515625\n",
      "\n",
      "After each iteration time step 8 V:\n",
      " [[ 0.  -5.2 -7.  -7.5]\n",
      " [-5.2 -6.5 -7.1 -7. ]\n",
      " [-7.  -7.1 -6.5 -5.2]\n",
      " [-7.5 -7.  -5.2  0. ]]\n",
      "\n",
      "Delta:  0.770751953125\n",
      "\n",
      "After each iteration time step 9 V:\n",
      " [[ 0.  -5.7 -7.7 -8.2]\n",
      " [-5.7 -7.2 -7.8 -7.7]\n",
      " [-7.7 -7.8 -7.2 -5.7]\n",
      " [-8.2 -7.7 -5.7  0. ]]\n",
      "\n",
      "Delta:  0.730255126953125\n",
      "\n",
      "After each iteration time step 10 V:\n",
      " [[ 0.  -6.1 -8.4 -9. ]\n",
      " [-6.1 -7.7 -8.4 -8.4]\n",
      " [-8.4 -8.4 -7.7 -6.1]\n",
      " [-9.  -8.4 -6.1  0. ]]\n",
      "\n",
      "Every iteration improved the value function\n"
     ]
    }
   ],
   "source": [
    "#Iterative Policy evaluation\n",
    "#4x4 Gid world\n",
    "# Non terminal states = S = {1,2,3,...14}\n",
    "# Actions = {up, down, right, left}\n",
    "\n",
    "# Parameters\n",
    "R = -1 # Reward\n",
    "# Given its undiscounted episodic task\n",
    "Y = 1 # gamma = 1\n",
    "act = ['u','d','r','l'] # action array\n",
    "# Grid state update array V and V'\n",
    "V = np.zeros([4,4])\n",
    "Vd = np.zeros([4,4])\n",
    "# termination value of loop to get vpi\n",
    "theta = 0.001\n",
    "delta = 0\n",
    "t = 0 # time step\n",
    "\n",
    "# To log output for 10 iterations\n",
    "iter = 10\n",
    "while(t<iter):\n",
    "    \n",
    "    for ix in range(0,4):\n",
    "        ys = 0 if ix>0 else 1\n",
    "        ye = 4 if ix<3 else 3\n",
    "        for iy in range(ys,ye):        \n",
    "            \n",
    "            # Selecting from array V\n",
    "            up    = V[ix-1][iy] if ix>0 else V[ix][iy]\n",
    "            down  = V[ix+1][iy] if ix<3 else V[ix][iy]\n",
    "            left  = V[ix][iy-1] if iy>0 else V[ix][iy]\n",
    "            right = V[ix][iy+1] if iy<3 else V[ix][iy]\n",
    "            m = [up,down,left,right] # storing in array\n",
    "            s_val = 0\n",
    "            # Calculate expected value\n",
    "            for i in m:\n",
    "                s_val += 0.25 *(R + i)\n",
    "            #Update new array V'\n",
    "            Vd[ix][iy] = s_val\n",
    "            \n",
    "    # Calculate largest updated value\n",
    "    delta = np.max(np.abs(V-Vd))\n",
    "    print('\\nDelta: ', delta)\n",
    "    # Update values from V' into old values array V\n",
    "    V = np.copy(Vd)\n",
    "    # Check if delta converges to theta\n",
    "    t+=1\n",
    "    print('\\nAfter each iteration time step %d V:\\n'%t,np.round(V,1))\n",
    "print('\\nEvery iteration improved the value function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta (0.00098) < theta (0.001)\n",
      "After time step t=131 value V ~ V(pi) \n",
      "V:\n",
      " [[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n"
     ]
    }
   ],
   "source": [
    "def policy_eval(Vd):\n",
    "    R = -1 # Reward\n",
    "    Y = 1 # gamma = 1\n",
    "    theta = 0.001\n",
    "    delta = 0\n",
    "    t = 0 # time step\n",
    "    V = np.copy(Vd)\n",
    "    t = 0 # time step\n",
    "    while(1):\n",
    "        for ix in range(0,4):\n",
    "            ys = 0 if ix>0 else 1\n",
    "            ye = 4 if ix<3 else 3\n",
    "            for iy in range(ys,ye):        \n",
    "\n",
    "                # Selecting from array V\n",
    "                up    = V[ix-1][iy] if ix>0 else V[ix][iy]\n",
    "                down  = V[ix+1][iy] if ix<3 else V[ix][iy]\n",
    "                left  = V[ix][iy-1] if iy>0 else V[ix][iy]\n",
    "                right = V[ix][iy+1] if iy<3 else V[ix][iy]\n",
    "                m = [up,down,left,right] # storing in array\n",
    "                s_val = 0\n",
    "                # Calculate expected value\n",
    "                for i in m:\n",
    "                    s_val += 0.25 *(R + i)\n",
    "                #Update new array V'\n",
    "                Vd[ix][iy] = s_val\n",
    "\n",
    "        # Calculate largest updated value\n",
    "        delta = np.max(np.abs(V-Vd))\n",
    "        # Update values from V' into old values array V\n",
    "        V = np.copy(Vd)\n",
    "        # Check if delta converges to theta\n",
    "        t+=1\n",
    "        if(delta < theta):\n",
    "            break\n",
    "    print('Delta (%.5f) < theta (%.3f)'%(delta,theta))\n",
    "    print('After time step t=%d value V ~ V(pi) \\nV:\\n'%t,np.round(V,1))\n",
    "    return V\n",
    "Vd = np.zeros([4,4])\n",
    "V = policy_eval(Vd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta (0.00097) < theta (0.001)\n",
      "After time step t=88 value V ~ V(pi) \n",
      "V:\n",
      " [[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "\n",
      " Using only V' it converges faster in lesser time step\n"
     ]
    }
   ],
   "source": [
    "# Policy iteration\n",
    "\n",
    "V = np.zeros([4,4])\n",
    "Vd = np.zeros([4,4])\n",
    "t = 0 # time step\n",
    "while(1):\n",
    "    for ix in range(0,4):\n",
    "        ys = 0 if ix>0 else 1\n",
    "        ye = 4 if ix<3 else 3\n",
    "        for iy in range(ys,ye):        \n",
    "            \n",
    "            # Selecting from array V\n",
    "            up    = Vd[ix-1][iy] if ix>0 else Vd[ix][iy]\n",
    "            down  = Vd[ix+1][iy] if ix<3 else Vd[ix][iy]\n",
    "            left  = Vd[ix][iy-1] if iy>0 else Vd[ix][iy]\n",
    "            right = Vd[ix][iy+1] if iy<3 else Vd[ix][iy]\n",
    "            m = [up,down,left,right] # storing in array\n",
    "            s_val = 0\n",
    "            # Calculate expected value\n",
    "            for i in m:\n",
    "                s_val += 0.25 *(R + i)\n",
    "            #Update new array V'\n",
    "            Vd[ix][iy] = s_val\n",
    "            \n",
    "    # Calculate largest updated value\n",
    "    delta = np.max(np.abs(V-Vd))\n",
    "    # Update values from V' into old values array V\n",
    "    V = np.copy(Vd)\n",
    "    # Check if delta converges to theta\n",
    "    t+=1\n",
    "    if(delta < theta):\n",
    "        break\n",
    "print('Delta (%.5f) < theta (%.3f)'%(delta,theta))\n",
    "print('After time step t=%d value V ~ V(pi) \\nV:\\n'%t,np.round(V,1))\n",
    "print('\\n Using only V\\' it converges faster in lesser time step' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Policy iteration and improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta (0.00098) < theta (0.001)\n",
      "After time step t=131 value V ~ V(pi) \n",
      "V:\n",
      " [[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "\n",
      " After 1 improvement:\n",
      " [[  0.   0. -14. -20.]\n",
      " [  0. -14. -18. -14.]\n",
      " [-14. -18. -14.   0.]\n",
      " [-20. -14.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "# Policy evaluation\n",
    "# Policy improvement\n",
    "\n",
    "def policy_improv(Vd):\n",
    "    V = np.copy(Vd)\n",
    "    policy_stable = True\n",
    "    for ix in range(0,4):\n",
    "        ys = 0 if ix>0 else 1\n",
    "        ye = 4 if ix<3 else 3\n",
    "        for iy in range(ys,ye):        \n",
    "\n",
    "                # Selecting from array V\n",
    "            up    = V[ix-1][iy] if ix>0 else V[ix][iy]\n",
    "            down  = V[ix+1][iy] if ix<3 else V[ix][iy]\n",
    "            left  = V[ix][iy-1] if iy>0 else V[ix][iy]\n",
    "            right = V[ix][iy+1] if iy<3 else V[ix][iy]\n",
    "            m = [up,down,left,right] # storing in array\n",
    "            s_val = 0\n",
    "            # Calculate action which returns max value\n",
    "            s_val = np.max(m)\n",
    "            #Update new array V'\n",
    "            Vd[ix][iy] = s_val\n",
    "    return Vd\n",
    "\n",
    "Vp = np.zeros([4,4])\n",
    "\n",
    "for i in range(1,2):\n",
    "    Ve = policy_eval(Vp)\n",
    "    Vp = policy_improv(Ve)\n",
    "    print('\\n After %d improvement:\\n'%i,np.round(Vp,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 8. When we defined a Markov Decision Process (MDP) we explicitly captured, using probability mass functions (PMF), the fact that the random variable R t +1 is dependent on the state S t and the action A t . Is the random variable R t +2 dependent on S t and A t ? Support your answer using the PMFs we used to define a MDP. [Hint: Start with the conditional PMF of R t +2 conditioned on S t and A t .]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Based on the markov property considering rewards are received at discrete time steps at sequential stages.\n",
    "\n",
    "Yes Reward R(t+2) depends on St and At.\n",
    "PMF of [ R(t+2) | St = s1, At = a1 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 10. We know that the state-value function v π ( s ) = E [ G t |S t = s ]. Use this definition of v π ( s ) to derive the Bellman equation for v π ( s ) for all s ∈ S . The Bellman equation will use the PMF corresponding to the policy π and the PMF p ( s 0 , r|s, a ) and will provide a recursive method of calculating v π ( s )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v π(s) = Eπ[Gt|St=s]\n",
    "from Gt = R(t+1) + Y*G(t+1) put in above equation\n",
    "\n",
    "v π(s) = Eπ[R(t+1) + Y*G(t+1)|St=s]\n",
    "\n",
    "v π(s) = Eπ[R(t+1)|St=s] + Y*Eπ[G(t+1)|St=s]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 11. Suppose an agent receives the sequence of rewards R 1 = 2, R 2 = − 1, R 3 = 10, and R 4 = − 3. Calculate the γ discounted return/reward for each time step for γ = 0 . 5. Also show that, if an agent receives a constant reward c , at every time step, for γ < 1, the infinite horizon discounted return is given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G0 3.625\n",
      "G1 3.25\n",
      "G2 8.5\n",
      "G3 -3\n",
      "Gt 2c\n"
     ]
    }
   ],
   "source": [
    "#Considering the rewards\n",
    "#Return Gt = R(t+1) + Y*R(t+2) + Y^2*R(t+3) + Y^3*R(t+4)\n",
    "\n",
    "# Rewards\n",
    "R1 = 2\n",
    "R2 = -1\n",
    "R3 = 10\n",
    "R4 = -3\n",
    "#discounted factor\n",
    "Y = 0.5\n",
    "\n",
    "#Rewards For each time step\n",
    "\n",
    "G0 = R1 + Y*R2 + Y**2*R3 + Y**3*R4\n",
    "print('G0',G0)\n",
    "\n",
    "G1 = R2 + Y*R3 + Y**2*R4\n",
    "print('G1',G1)\n",
    "\n",
    "G2 = R3 + Y*R4\n",
    "print('G2',G2)\n",
    "\n",
    "G3 = R4\n",
    "print('G3',G3)\n",
    "\n",
    "# If Y<1, receiveing a constant reward\n",
    "#Gt = c + Y*c + Y^2*c + Y^3*c + Y^4*c + Y^5*c + .....\n",
    "#Gt = c*(1 + Y + Y^2 + Y^3 + Y^4 + Y^5 + ....) \n",
    "#Using sum to infinity its given by\n",
    "#Gt = c * 1/ (1-Y) # Put Y = 0.5\n",
    "#Gt = 2*c\n",
    "\n",
    "# Infinite horizon discounted return\n",
    "Gt = '2c'\n",
    "print('Gt',Gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 12. Suppose you are given the optimal state-value function v ∗ ( s ), for all s ∈ S . How will you find the optimal policy? Show your steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Given v*(s) optimal state value function\n",
    "0. v*(s) = max vπ(s) from (3.15)\n",
    "1. #From Bellman optimality equation we have\n",
    "2. v*(s) = max(a) Σ[s',r] p(s',r|s,a)[r+Yv*(s')]\n",
    "3. To find π*(s), from 4.9 we have\n",
    "4. π'(s) = argmax(a)Σ p(s',R|s,a)[r + Y*v π(s')]\n",
    "5. therefore for optimal policy, each action return the maximum value which is optimal value function\n",
    "6. π*(s) = argmax(a)Σ p(s',R|s,a)[r + Yv*(s')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 13. A server requires information from a sensor. The server would like the information to be fresh. However, there is a cost to querying information from the sensor. Specifically, the state at the server can be either fresh or stale. The former indicates that the information at the server about the sensor is fresh and the latter indicates that it is stale. At any time, the server may choose to query or remain silent. A query makes stale information fresh with probability 0 . 8 and fresh information stale with probability 0 . 1. Staying silent keeps stale information stale with probability 1 and makes fresh information stale with probability 0 . 5. Also, a query when current information at the server is stale costs 8 dollars. Otherwise, it costs 4 dollars. Staying quiet has a reward of 4 dollars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. Draw and/or tabulate the MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For server\n",
    "* States = {stale, fresh}\n",
    "* Actions = {query, silent}\n",
    "\n",
    "\n",
    "\n",
    "| S no. | State(s) | Action(a) | State(s') | Probability (p) | Reward (r)|\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "|1. | Stale | Querry | Fresh | 0.8 | -8 |\n",
    "|2. | Stale | Querry | Stale | 0.2 | -8 |\n",
    "|3. | Stale | Silent | Stale | 1 | 4 |\n",
    "|4. | Fresh | Querry | Stale | 0.1 | -4 |\n",
    "|5. | Fresh | Querry | Fresh | 0.9 | -4 |\n",
    "|6. | Fresh | Silent | Fresh | 0.5 | 4 |\n",
    "|7. | Fresh | Silent | Stale | 0.5 | 4 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. Consider a finite horizon problem in which the server gets to optimize costs over three time steps. Assume that if at the end of the three time steps the server has stale information it pays a cost of 10, else it receives a reward of 10. Also, assume that future costs are discounted with a factor 1 / 2. Calculate the optimal costs for every starting state. Calculate the optimal policy. Show all your steps. Guesses and intuition will bring you unbounded costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solved in uploaded pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3. Suppose the server is to optimize for ever and is looking for a stationary policy. Write down the first four iterations of value iteration that will help the server get closer to a stationary policy. Do the same for policy iteration (where one iteration includes evaluation and improvement). Clearly identify each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solved in uploaded pdf.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 14. Assume an infinite horizon discounted costs problem. Prove that the policy improvement step either improves the current policy or the current policy is optimal. Show all your steps and support them using the properties of dynamic programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Policy improvement in infinite horizon discounted costs problem.\n",
    "\n",
    "#Proof.\n",
    "#From infinite horizon discounted costs expected return becomes a constant\n",
    "#From 4.7 considering policies π and π' and 4.8 such that we have\n",
    "If q π(s,π'(s)) >=v π(s) then vπ'(s) >= vπ(s)\n",
    "\n",
    "v π(s) <= E π'[ R(t+1) + YR(t+2) + Y^2*R(t+3) + Y^3R(t+4) + ... | St = s]\n",
    "#Considering All the rewards as R for our grid world example, we have\n",
    "0. v π(s) <= E π'[ R + YR + Y^2*R + Y^3R + ... | St = s]\n",
    "1. v π(s) <= E π'[ R/(1-Y) | St = s], from horizon discounted costs\n",
    "2. Considering R/(1-Y) = some constant C\n",
    "3. v π(s) <= E π'[C | St = s] = C* E π'[St = s]\n",
    "4. v π(s) <= v π'(s)\n",
    "5. Similar from Q11, as Expected return Gt = 2c\n",
    "6. π'(s) = argmax(a)Σ p(s',R|s,a)[r + Y*vπ(s')]\n",
    "7.  π'(s) = argmax(a)Σ p(s',R|s,a)[r + Y*C]\n",
    "8. If π' is better than π, π'(s) > π(s)\n",
    "9. If π' is equal to π, then v π = v π'\n",
    "10. So, we obtain v π'(s) = max Σ p(s',R|s,a)[r + Y*C]\n",
    "11. Hence, the policy improvement step is either better or the current policy will be optimal.\n",
    "12. π'(s) <-- argmax(a)Σ p(s',R|s,a)[r + Y*v π'(s)]\n",
    "13. So old-action != π'(s)m then policy stable = False or old-action = π'(s), policy stable = True\n",
    "14. V ~ v* and π = π*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 15. The Stairwell: A stairwell connects the ground floor (G) of a palace with its first floor (F). It consists of a total of n steps, indexed 1 , 2 , . . . , n , with step 1 close to G and step n connected to F. A robot may start in any step and must learn to decide on whether it should go to F or to G. Both F and G are terminal states. The robot gets a reward of 2 with probability 0 . 5, and 0 otherwise, when it descends from step i to i− 1, 1 < i ≤ n . It also gets a reward of 1 on going from step 1 to G. On the other hand, the robot pays a cost of 1 (reward − 1) when it moves from step i to i + 1, 1 ≤ i < n . Finally, the robot gets a bumper reward of 2 n when it goes from step n to F. The robot can only take one step at a time. Start with a policy in which the robot goes up or down a step in the stairwell with probability 0 . 5. Draw the corresponding MDP for n = 2. Use policy iteration to find the optimal policy for n = 2. Show all your iterations and explain them. How does the number of iterations increase with n ? Explain your answer. Assume a discount factor of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solved in uploaded pdf."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
